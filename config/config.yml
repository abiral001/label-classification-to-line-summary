batch_size: 32
max_length: 384
device: "cuda"
n_epochs: 15
learning_rate: 0.005
dropout_prob: 0.2
weight_decay: 0.01
ckpt_dir: "./models/Meta-Llama-3-8B-Instruct/"
tokenizer_path: "./models/Meta-Llama-3-8B-Instruct/tokenizer.model"
tokenizer_lc: "roberta-large"